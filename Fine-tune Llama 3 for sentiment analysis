{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1192499,"sourceType":"datasetVersion","datasetId":622510},{"sourceId":33547,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28079,"modelId":39106}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installations and imports","metadata":{}},{"cell_type":"code","source":"!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n!pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n!pip install -q -U transformers==\"4.40.0\"\n!pip install -q -U accelerate\n!pip install -q -U datasets\n!pip install -q -U trl\n!pip install -q -U peft\n!pip install -q -U tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-07-10T02:23:06.986780Z","iopub.execute_input":"2024-07-10T02:23:06.987449Z","iopub.status.idle":"2024-07-10T02:25:23.818379Z","shell.execute_reply.started":"2024-07-10T02:23:06.987417Z","shell.execute_reply":"2024-07-10T02:25:23.817332Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ncudf 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\ntensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.17.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-07-10T02:25:23.820488Z","iopub.execute_input":"2024-07-10T02:25:23.820796Z","iopub.status.idle":"2024-07-10T02:25:23.825244Z","shell.execute_reply.started":"2024-07-10T02:25:23.820769Z","shell.execute_reply":"2024-07-10T02:25:23.824435Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-07-10T02:25:23.826193Z","iopub.execute_input":"2024-07-10T02:25:23.826532Z","iopub.status.idle":"2024-07-10T02:25:23.836035Z","shell.execute_reply.started":"2024-07-10T02:25:23.826500Z","shell.execute_reply":"2024-07-10T02:25:23.835122Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom trl import setup_chat_format\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split","metadata":{"papermill":{"duration":14.485002,"end_time":"2023-10-16T11:00:18.917449","exception":false,"start_time":"2023-10-16T11:00:04.432447","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-10T02:25:23.838010Z","iopub.execute_input":"2024-07-10T02:25:23.838291Z","iopub.status.idle":"2024-07-10T02:25:42.205604Z","shell.execute_reply.started":"2024-07-10T02:25:23.838269Z","shell.execute_reply":"2024-07-10T02:25:42.204740Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-07-10 02:25:34.047068: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-10 02:25:34.047172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-10 02:25:34.210294: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"pytorch version {torch.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:42:30.582648Z","iopub.execute_input":"2024-07-05T02:42:30.583422Z","iopub.status.idle":"2024-07-05T02:42:30.588198Z","shell.execute_reply.started":"2024-07-05T02:42:30.583395Z","shell.execute_reply":"2024-07-05T02:42:30.587194Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"pytorch version 2.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"working on {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-10T02:25:42.207074Z","iopub.execute_input":"2024-07-10T02:25:42.207976Z","iopub.status.idle":"2024-07-10T02:25:42.212980Z","shell.execute_reply.started":"2024-07-10T02:25:42.207947Z","shell.execute_reply":"2024-07-10T02:25:42.212092Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"working on cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T02:25:42.213978Z","iopub.execute_input":"2024-07-10T02:25:42.214248Z","iopub.status.idle":"2024-07-10T02:25:42.241193Z","shell.execute_reply.started":"2024-07-10T02:25:42.214225Z","shell.execute_reply":"2024-07-10T02:25:42.240348Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Preparing the data and the core evaluation functions","metadata":{}},{"cell_type":"code","source":"filename = \"../input/sentiment-analysis-for-financial-news/all-data.csv\"\n\ndf = pd.read_csv(filename, \n                 names=[\"sentiment\", \"text\"],\n                 encoding=\"utf-8\", encoding_errors=\"replace\")\n\nX_train = list()\nX_test = list()\nfor sentiment in [\"positive\", \"neutral\", \"negative\"]:\n    train, test  = train_test_split(df[df.sentiment==sentiment], \n                                    train_size=300,\n                                    test_size=300, \n                                    random_state=42)\n    X_train.append(train)\n    X_test.append(test)\n\nX_train = pd.concat(X_train).sample(frac=1, random_state=10)\nX_test = pd.concat(X_test)\n\neval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\nX_eval = df[df.index.isin(eval_idx)]\nX_eval = (X_eval\n          .groupby('sentiment', group_keys=False)\n          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\nX_train = X_train.reset_index(drop=True)\n\ndef generate_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n            \"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = \"\"\".strip()\n\nX_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n                       columns=[\"text\"])\nX_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n                      columns=[\"text\"])\n\ny_true = X_test.sentiment\nX_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n\ntrain_data = Dataset.from_pandas(X_train)\neval_data = Dataset.from_pandas(X_eval)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T02:25:42.242412Z","iopub.execute_input":"2024-07-10T02:25:42.242675Z","iopub.status.idle":"2024-07-10T02:25:43.580531Z","shell.execute_reply.started":"2024-07-10T02:25:42.242653Z","shell.execute_reply":"2024-07-10T02:25:43.579500Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def evaluate(y_true, y_pred):\n    labels = ['positive', 'neutral', 'negative']\n    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n    def map_func(x):\n        return mapping.get(x, 1)\n    \n    y_true = np.vectorize(map_func)(y_true)\n    y_pred = np.vectorize(map_func)(y_pred)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n    \n    # Generate accuracy report\n    unique_labels = set(y_true)  # Get unique labels\n    \n    for label in unique_labels:\n        label_indices = [i for i in range(len(y_true)) \n                         if y_true[i] == label]\n        label_y_true = [y_true[i] for i in label_indices]\n        label_y_pred = [y_pred[i] for i in label_indices]\n        accuracy = accuracy_score(label_y_true, label_y_pred)\n        print(f'Accuracy for label {label}: {accuracy:.3f}')\n        \n    # Generate classification report\n    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n    print('\\nClassification Report:')\n    print(class_report)\n    \n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n    print('\\nConfusion Matrix:')\n    print(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T02:25:43.581915Z","iopub.execute_input":"2024-07-10T02:25:43.582368Z","iopub.status.idle":"2024-07-10T02:25:43.591912Z","shell.execute_reply.started":"2024-07-10T02:25:43.582336Z","shell.execute_reply":"2024-07-10T02:25:43.590973Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Testing the various models ","metadata":{}},{"cell_type":"markdown","source":"**llama2**","metadata":{}},{"cell_type":"code","source":"model_name = \"Orkhan/llama-2-7b-absa\"\n\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n    temperature = 0.01\n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\nmax_seq_length = 512 #2048\ntokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\ny_pred = predict(test, model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:09:56.615906Z","iopub.execute_input":"2024-07-05T07:09:56.616599Z","iopub.status.idle":"2024-07-05T07:15:48.322868Z","shell.execute_reply.started":"2024-07-05T07:09:56.616567Z","shell.execute_reply":"2024-07-05T07:15:48.322002Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03abd6a0f8724d83ab74b9bb741172c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a998322438c4da9907f024c744daede"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 900/900 [05:41<00:00,  2.63it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.538\nAccuracy for label 0: 0.540\nAccuracy for label 1: 0.087\nAccuracy for label 2: 0.987\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.54      0.67       300\n           1       0.39      0.09      0.14       300\n           2       0.46      0.99      0.62       300\n\n    accuracy                           0.54       900\n   macro avg       0.58      0.54      0.48       900\nweighted avg       0.58      0.54      0.48       900\n\n\nConfusion Matrix:\n[[162  39  99]\n [ 19  26 255]\n [  2   2 296]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Fine-tuned RoBERT**","metadata":{}},{"cell_type":"code","source":"model_name = \"FacebookAI/roberta-large-mnli\"\n\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n    temperature = 0.01\n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\nmax_seq_length = 512 #2048\ntokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\ny_pred = predict(test, model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:16:22.813784Z","iopub.execute_input":"2024-07-05T07:16:22.814198Z","iopub.status.idle":"2024-07-05T07:17:23.571197Z","shell.execute_reply.started":"2024-07-05T07:16:22.814167Z","shell.execute_reply":"2024-07-05T07:17:23.570281Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7247035e84a44fd59c8492ed1f58df2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7094abe605fa4b16855b8be235c79b0a"}},"metadata":{}},{"name":"stderr","text":"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\nSome weights of RobertaForCausalLM were not initialized from the model checkpoint at FacebookAI/roberta-large-mnli and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"335fa5dfb8e448eeb6bebc13516330ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71c3337bba134fe5b0a5e12822e1f5f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de8649ecbeef482bbb53fa11182dfd0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f0e71ae56004ff6bb68977f1b2404ca"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 900/900 [00:50<00:00, 17.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.333\nAccuracy for label 0: 0.000\nAccuracy for label 1: 1.000\nAccuracy for label 2: 0.000\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       300\n           1       0.33      1.00      0.50       300\n           2       0.00      0.00      0.00       300\n\n    accuracy                           0.33       900\n   macro avg       0.11      0.33      0.17       900\nweighted avg       0.11      0.33      0.17       900\n\n\nConfusion Matrix:\n[[  0 300   0]\n [  0 300   0]\n [  0 300   0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model_name = \"michellejieli/emotion_text_classifier\"\n\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n    temperature = 0.01\n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\nmax_seq_length = 512 #2048\ntokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\ny_pred = predict(test, model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:18:16.978456Z","iopub.execute_input":"2024-07-05T07:18:16.979247Z","iopub.status.idle":"2024-07-05T07:18:40.217463Z","shell.execute_reply.started":"2024-07-05T07:18:16.979208Z","shell.execute_reply":"2024-07-05T07:18:40.216539Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"816500adafde4412b2f5233e9f1d6bf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d621f7fdc43e47e98db59f3a62e24f55"}},"metadata":{}},{"name":"stderr","text":"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\nSome weights of RobertaForCausalLM were not initialized from the model checkpoint at michellejieli/emotion_text_classifier and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/413 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71062654001443aba53b51fdc5d9e5d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bc9c509fdb040c7832ca0032f7f2819"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6ac68e79c6446b7a183f36922d8605a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304c30378ff9464d888715b1c9f2853f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e1adfecfe246ffbe07ad6ff0d6996d"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 900/900 [00:18<00:00, 48.94it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.333\nAccuracy for label 0: 0.000\nAccuracy for label 1: 1.000\nAccuracy for label 2: 0.000\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       300\n           1       0.33      1.00      0.50       300\n           2       0.00      0.00      0.00       300\n\n    accuracy                           0.33       900\n   macro avg       0.11      0.33      0.17       900\nweighted avg       0.11      0.33      0.17       900\n\n\nConfusion Matrix:\n[[  0 300   0]\n [  0 300   0]\n [  0 300   0]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**llama3 without fine-tuning**","metadata":{}},{"cell_type":"code","source":"model_name = \"/kaggle/input/llama-3/transformers/8b-hf/1\"\n\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\nmax_seq_length = 512 #2048\ntokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-07-10T02:25:43.594300Z","iopub.execute_input":"2024-07-10T02:25:43.594609Z","iopub.status.idle":"2024-07-10T02:27:39.360346Z","shell.execute_reply.started":"2024-07-10T02:25:43.594574Z","shell.execute_reply":"2024-07-10T02:27:39.359426Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a9b25f90b4f4c4cbcf808a3ea9c91f1"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict(test, model, tokenizer):\n    y_pred = []\n    for i in tqdm(range(len(X_test))):\n        prompt = X_test.iloc[i][\"text\"]\n        pipe = pipeline(task=\"text-generation\", \n                        model=model, \n                        tokenizer=tokenizer, \n                        max_new_tokens = 1, \n                        temperature = 0.0,\n                       )\n        result = pipe(prompt)\n        answer = result[0]['generated_text'].split(\"=\")[-1]\n        if \"positive\" in answer:\n            y_pred.append(\"positive\")\n        elif \"negative\" in answer:\n            y_pred.append(\"negative\")\n        elif \"neutral\" in answer:\n            y_pred.append(\"neutral\")\n        else:\n            y_pred.append(\"none\")\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-07-10T02:27:39.361515Z","iopub.execute_input":"2024-07-10T02:27:39.361781Z","iopub.status.idle":"2024-07-10T02:27:39.369211Z","shell.execute_reply.started":"2024-07-10T02:27:39.361758Z","shell.execute_reply":"2024-07-10T02:27:39.368162Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"y_pred = predict(test, model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T03:12:12.099801Z","iopub.execute_input":"2024-07-05T03:12:12.100162Z","iopub.status.idle":"2024-07-05T03:17:41.183282Z","shell.execute_reply.started":"2024-07-05T03:12:12.100135Z","shell.execute_reply":"2024-07-05T03:17:41.182381Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|██████████| 900/900 [05:29<00:00,  2.73it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T03:17:47.359478Z","iopub.execute_input":"2024-07-05T03:17:47.359863Z","iopub.status.idle":"2024-07-05T03:17:47.382822Z","shell.execute_reply.started":"2024-07-05T03:17:47.359834Z","shell.execute_reply":"2024-07-05T03:17:47.381864Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Accuracy: 0.354\nAccuracy for label 0: 0.020\nAccuracy for label 1: 0.243\nAccuracy for label 2: 0.800\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.86      0.02      0.04       300\n           1       0.19      0.24      0.22       300\n           2       0.46      0.80      0.59       300\n\n    accuracy                           0.35       900\n   macro avg       0.51      0.35      0.28       900\nweighted avg       0.51      0.35      0.28       900\n\n\nConfusion Matrix:\n[[  6 243  51]\n [  1  73 226]\n [  0  60 240]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fine-tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import (accuracy_score, \n                             recall_score, \n                             precision_score, \n                             f1_score)\n\nfrom transformers import EarlyStoppingCallback, IntervalStrategy\n\ndef compute_metrics(p):    \n    pred, labels = p\n    pred = np.argmax(pred, axis=1)\n    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n    recall = recall_score(y_true=labels, y_pred=pred)\n    precision = precision_score(y_true=labels, y_pred=pred)\n    f1 = f1_score(y_true=labels, y_pred=pred)    \n    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}","metadata":{"execution":{"iopub.status.busy":"2024-07-10T02:27:39.370408Z","iopub.execute_input":"2024-07-10T02:27:39.370723Z","iopub.status.idle":"2024-07-10T02:27:39.382947Z","shell.execute_reply.started":"2024-07-10T02:27:39.370699Z","shell.execute_reply":"2024-07-10T02:27:39.382054Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"output_dir=\"trained_weigths\"\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,                    # directory to save and repository id\n    num_train_epochs=5,                       # number of training epochs\n    per_device_train_batch_size=1,            # batch size per device during training\n    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    logging_steps=25,                         # log every 10 steps\n    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n    max_steps=-1,\n    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n    group_by_length=False,\n    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n    report_to=\"tensorboard\",                  # report metrics to tensorboard\n    #evaluation_strategy=\"steps\",              # save checkpoint every epoch\n    #load_best_model_at_end = True,\n    #eval_steps = 25,\n    #metric_for_best_model = 'accuracy',\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=train_data,\n    #eval_dataset=eval_data,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=max_seq_length,\n    packing=False,\n    dataset_kwargs={\n        \"add_special_tokens\": False,\n        \"append_concat_token\": False,\n    },\n    #compute_metrics=compute_metrics,\n    #callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T02:27:39.385063Z","iopub.execute_input":"2024-07-10T02:27:39.385411Z","iopub.status.idle":"2024-07-10T02:27:42.382807Z","shell.execute_reply.started":"2024-07-10T02:27:39.385382Z","shell.execute_reply":"2024-07-10T02:27:42.382047Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dc2b7114a4a44f380962b3c6cd0c0c0"}},"metadata":{}}]},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-09T03:10:23.370096Z","iopub.execute_input":"2024-07-09T03:10:23.370646Z","iopub.status.idle":"2024-07-09T05:09:15.476524Z","shell.execute_reply.started":"2024-07-09T03:10:23.370608Z","shell.execute_reply":"2024-07-09T05:09:15.475527Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [560/560 1:58:37, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.763900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.947900</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.876900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.846800</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.823800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.757000</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.695100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.695100</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.684900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.481000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.467200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.485200</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.493900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.377600</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.299400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.275600</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.292400</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.285800</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.207100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.191600</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.185400</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.187800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=560, training_loss=0.5537229256970542, metrics={'train_runtime': 7131.0237, 'train_samples_per_second': 0.631, 'train_steps_per_second': 0.079, 'total_flos': 1.7183004555264e+16, 'train_loss': 0.5537229256970542, 'epoch': 4.977777777777778})"},"metadata":{}}]},{"cell_type":"code","source":"# Save trained model and tokenizer\ntrainer.save_model()\ntokenizer.save_pretrained(output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T05:09:17.366048Z","iopub.execute_input":"2024-07-09T05:09:17.366697Z","iopub.status.idle":"2024-07-09T05:09:19.333627Z","shell.execute_reply.started":"2024-07-09T05:09:17.366667Z","shell.execute_reply":"2024-07-09T05:09:19.332426Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"('trained_weigths/tokenizer_config.json',\n 'trained_weigths/special_tokens_map.json',\n 'trained_weigths/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# %load_ext tensorboard\n# %tensorboard --logdir logs/runs","metadata":{"execution":{"iopub.status.busy":"2024-07-09T03:02:43.080939Z","iopub.status.idle":"2024-07-09T03:02:43.081310Z","shell.execute_reply.started":"2024-07-09T03:02:43.081125Z","shell.execute_reply":"2024-07-09T03:02:43.081140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"y_pred = predict(test, model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T03:02:43.082517Z","iopub.status.idle":"2024-07-09T03:02:43.082858Z","shell.execute_reply.started":"2024-07-09T03:02:43.082683Z","shell.execute_reply":"2024-07-09T03:02:43.082696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation = pd.DataFrame({'text': X_test[\"text\"], \n                           'y_true':y_true, \n                           'y_pred': y_pred},\n                         )\nevaluation.to_csv(\"test_predictions.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T22:27:57.741189Z","iopub.status.idle":"2024-06-19T22:27:57.741639Z","shell.execute_reply.started":"2024-06-19T22:27:57.741407Z","shell.execute_reply":"2024-06-19T22:27:57.741426Z"},"trusted":true},"execution_count":null,"outputs":[]}]}